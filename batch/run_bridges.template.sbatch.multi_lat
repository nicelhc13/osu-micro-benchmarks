#!/bin/bash
#SBATCH --gres=gpu:p100:4
#SBATCH --ntasks 2
#SBATCH --ntasks-per-node=1
#SBATCH -N 2
#SBATCH -p gpu 
#SBATCH -o multi_lat.biggermem.out
#SBATCH -t 00:15:00
#SBATCH --mail-user=hochan@utexas.edu
#SBATCH --mail-type=fail
#SBATCH --mail-type=end

nvidia-smi

source ~/.bash_profile

module list
RUN=mpirun
mpirun --version

export HFILE=`generate_pbs_nodefile`
cat $HFILE | sort -u > hosts.txt.$SLURM_JOBID
export OMP_NUM_THREADS=1
export MV2_USE_CUDA=1
export MV2_USE_GDR=1
export MPI_THREAD_MULTIPLE=1
export MV2_CPU_MAPPING=0
export MV2_ENABLE_AFFINITY=0
export LD_PRELOAD=/home/hlee89/bin/opt/mvapich2/gdr/2.3.4/mcast/no-openacc/cuda10.1/mofed4.7/mpirun/gnu4.8.5/lib64/libmpi.so
export MV2_DEBUG_SHOW_BACKTRACE=1
export MV2_SHOW_ENV_INFO=1

echo "Device to Device (Lt)"
mpirun -np 2 ./osu_latency_mt -m 0:40000000 -t 8:8 D D

echo "Host to Host (Lt)"
mpirun -np 2 ./osu_latency_mt -m 0:40000000 -t 8:8 H H

echo "Host to Device (Lt)"
mpirun -np 2 ./osu_latency_mt -m 0:40000000 -t 8:8 H D

echo "Device to Host (Lt)"
mpirun -np 2 ./osu_latency_mt -m 0:40000000 -t 8:8 D H

echo "Device to Device (sLt)"
mpirun -np 2 ./osu_latency_mt -m 0:40000000 -t 1:1 D D

echo "Host to Host (sLt)"
mpirun -np 2 ./osu_latency_mt -m 0:40000000 -t 1:1 H H

echo "Host to Device (sLt)"
mpirun -np 2 ./osu_latency_mt -m 0:40000000 -t 1:1 H D

echo "Device to Host (sLt)"
mpirun -np 2 ./osu_latency_mt -m 0:40000000 -t 1:1 D H



rm hosts.txt.$SLURM_JOBID
